version: "3.8"
services:
  ollama:
    image: ollama/ollama
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    # Optionally, pull a model on startup (uncomment and adjust as needed):
    # entrypoint: ["/bin/sh", "-c", "ollama serve & sleep 5 && ollama pull llama2:7b && wait"]

  backend:
    build: .
    container_name: backend
    environment:
      - OLLAMA_URL=http://ollama:11434
      # Add any other env vars your backend needs here
    ports:
      - "3000:3000" # Change if your backend uses a different port
    depends_on:
      - ollama
    volumes:
      - .:/app # For local dev: mount your code for hot-reloading
    # command: npm run dev # For local dev
    # command: npm run start # For production (uncomment and adjust as needed)

volumes:
  ollama-data:
